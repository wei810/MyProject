{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries and Load Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.device_count()\n",
    "import os\n",
    "import sys; sys.path.append('../')\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from PIL import Image\n",
    "from torchvision.utils import make_grid\n",
    "import numpy as np\n",
    "from torchvision import datasets, models, transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.transforms import *\n",
    "import pickle\n",
    "from data_wrapper import Data\n",
    "from utils import *\n",
    "from models import *\n",
    "import cv2\n",
    "from tqdm.notebook import tqdm_notebook\n",
    "from matplotlib import image as mpimg\n",
    "from matplotlib import pyplot as plt\n",
    "from IPython.display import display\n",
    "from fid_score.fid_score import FidScore\n",
    "with open('../coco_fileDict.p', 'rb') as f:\n",
    "    fileDict = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda:0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "state_dicts = torch.load('./state_dicts/result.pt', map_location=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {}\n",
    "generator = Unet(UnetEncoder, UnetDecoder)\n",
    "models['gen'] = generator\n",
    "models['gen'].load_state_dict(state_dicts['gen'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in models.keys():\n",
    "    for x in models[k].parameters():\n",
    "        x.requires_grad = False\n",
    "    models[k].eval()\n",
    "models['gen'].to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchinfo import summary\n",
    "summary(models['gen'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# define a function to convert grayscale images to rgb ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Union, List, Tuple\n",
    "from skimage.color import rgb2gray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cvtBW2RGB(x: np.ndarray, \n",
    "              size: Tuple[int] = (192, 192), \n",
    "              display: bool = True) -> np.ndarray :\n",
    "    x = np.array(x)\n",
    "    x = cv2.resize(x, size)\n",
    "    assert len(x.shape) == 3\n",
    "    if str(x.dtype).find('float')!=-1:\n",
    "        print('Data ranges from 0 to 1')\n",
    "    else:\n",
    "        print('Data ranges from 0 to 255')\n",
    "        x = x / 255.\n",
    "    x = torch.from_numpy(x)[None].permute(0, 3, 1, 2)\n",
    "    inp = (x.type(torch.float32) - 0.5)/0.5\n",
    "    y = models['gen'](inp.to(device)).to('cpu')*0.5 + 0.5\n",
    "    if display:\n",
    "        fig, axs = plt.subplots(len(x), 2, figsize=(15, 15))\n",
    "        if len(axs.shape) == 1:\n",
    "            axs = axs[None]\n",
    "        for i in range(len(x)):\n",
    "            axs[i, 0].imshow(np.array(ToPILImage()(x[i])))\n",
    "            axs[i, 1].imshow(np.array(ToPILImage()(y[i])))\n",
    "    plt.show()\n",
    "    y = np.array(y.permute(0, 2, 3, 1) * 255.).astype(np.uint8)\n",
    "    return y[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = mpimg.imread('./images/wyoming-scenery-wallpaper-2.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = cv2.cvtColor(x, cv2.COLOR_RGB2GRAY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.repeat(x[..., None], 3, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "2048/4/16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1172/4/16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = cvtBW2RGB(x, size=(16*32, 16*18))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = [0.01, 0.05, 0.1, 0.5, 1.0, 1.5, 2.0, 5.0, 10.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in test:\n",
    "    for name, module in models['gen'].named_modules():\n",
    "        if 'fusion' in name:\n",
    "            try:\n",
    "                getattr(module, 'apply_noise')\n",
    "                module.apply_noise = True\n",
    "                module.noise_params['stddev'] = i\n",
    "            except:\n",
    "                pass\n",
    "    y_list.append(cvtBW2RGB(x, size=(16*32, 16*18)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ys = np.array(y_list, int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(y_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(y_list) - 1):\n",
    "    plt.imshow((np.abs(ys[0] - ys[i+1]) >=2).astype(float))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Video Colorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, module in models['gen'].named_modules():\n",
    "    if 'fusion' in name:\n",
    "        try:\n",
    "            getattr(module, 'apply_noise')\n",
    "            module.apply_noise = False\n",
    "        except:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture('/home/ai2020c/4K Video (Ultra HD) Unbelievable Beauty.mp4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap.read()[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "640/16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture('/home/ai2020c/3 Hours of Amazing Nature Scenery & Relaxing Music for Stress Relief.mp4')\n",
    "size = (80*16, 44*16)\n",
    "bs = 1\n",
    "out_gen = cv2.VideoWriter('gen1.mp4',cv2.VideoWriter_fourcc(*'MP4V'), 30.0, size)\n",
    "out_rgb = cv2.VideoWriter('rgb1.mp4',cv2.VideoWriter_fourcc(*'MP4V'), 30.0, size)\n",
    "start = 20000\n",
    "interval = 10000\n",
    "f_stack = []\n",
    "k = 0\n",
    "for x in tqdm_notebook(range(start, start + interval)):\n",
    "    cap.set(1, x)\n",
    "    _, frame = cap.read()\n",
    "    f = cv2.resize(frame, size)\n",
    "    f = cv2.cvtColor(f, cv2.COLOR_BGR2RGB)\n",
    "    rgb_frame = f.astype(np.uint8)\n",
    "    out_rgb.write(cv2.cvtColor(rgb_frame, cv2.COLOR_RGB2BGR))\n",
    "    f = torch.from_numpy(f).permute(2, 0, 1) / 255.\n",
    "    f = (f - 0.5)/0.5\n",
    "    f = (Grayscale(3)(f)[None]).type(torch.float32)\n",
    "    f_stack.append(f)\n",
    "    if (x - start) % bs == bs - 1:\n",
    "        f_stack = torch.cat(f_stack)\n",
    "        gen_frame = models['gen'](f_stack.to(device)).cpu()*0.5 + 0.5\n",
    "        gen_frame = (gen_frame.permute(0, 2, 3, 1).detach().numpy()*255).astype(np.uint8)\n",
    "        for a in gen_frame:\n",
    "            out_gen.write(cv2.cvtColor(a, cv2.COLOR_RGB2BGR))\n",
    "        f_stack = []\n",
    "        if k%25==0:\n",
    "            print('rgb')\n",
    "            plt.imshow(rgb_frame)\n",
    "            plt.show()\n",
    "            print('gen')\n",
    "            plt.imshow(gen_frame[-1])\n",
    "            plt.show()\n",
    "        k += 1\n",
    "out_gen.release()\n",
    "out_rgb.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_gen.release()\n",
    "out_rgb.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf source_images_coco/ target_images_coco/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_result_real_fake(real, fake):\n",
    "    if torch.is_tensor(real) and torch.is_tensor(fake):\n",
    "        fig, axs = plt.subplots(1, 2, figsize=(20, 15))\n",
    "        axs[0].imshow(np.array(ToPILImage()(make_grid(fake))))\n",
    "        axs[0].set_title('fake')\n",
    "        axs[1].imshow(np.array(ToPILImage()(make_grid(real))))\n",
    "        axs[1].set_title('real')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, module in models['gen'].named_modules():\n",
    "    if 'fusion' in name:\n",
    "        try:\n",
    "            getattr(module, 'apply_noise')\n",
    "            module.apply_noise = False\n",
    "        except:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%mkdir source_images_coco/\n",
    "%mkdir target_images_coco/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = (128, 128)\n",
    "bs = 32\n",
    "ds = Data(fileDict['train'], 'val', size)\n",
    "train = DataLoader(ds, batch_size=bs)\n",
    "ds = Data(fileDict['val'], 'val', size)\n",
    "val = DataLoader(ds, batch_size=bs)\n",
    "dl = {'Train': train, 'Val': val}\n",
    "c_range_scale_factor = 1.\n",
    "for phase in ['Train', 'Val']:\n",
    "    print(phase)\n",
    "    for i, d in tqdm_notebook(enumerate(dl[phase])):\n",
    "        with torch.no_grad():\n",
    "            x = d['bw']\n",
    "            y = d['rgb']\n",
    "            x = x*c_range_scale_factor\n",
    "            fake = models['gen'](x.to(device))\n",
    "            x = x.cpu()*0.5 + 0.5\n",
    "            y = y.cpu()*0.5 + 0.5\n",
    "            fake = fake.cpu()*0.5 + 0.5\n",
    "            display_result_real_fake(y.cpu(), fake.cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "size = (128, 128)\n",
    "bs = 32\n",
    "ds = Data(fileDict['train'], 'val', size)\n",
    "train = DataLoader(ds, batch_size=bs)\n",
    "ds = Data(fileDict['val'], 'val', size)\n",
    "val = DataLoader(ds, batch_size=bs)\n",
    "ds = Data(fileDict['test'], 'test', size)\n",
    "test = DataLoader(ds, batch_size=bs)\n",
    "dl = {'Train': train, 'Val': val, 'Test': test}\n",
    "c_range_scale_factor = 1.\n",
    "for phase in ['Train', 'Val', 'Test']:\n",
    "    print(phase)\n",
    "    source_path = f'./source_images_coco/{size[0]}'\n",
    "    p = source_path + f'/{phase}'\n",
    "    %mkdir {source_path}\n",
    "    %mkdir {p}\n",
    "    target_path = f'./target_images_coco/{size[0]}'\n",
    "    p = target_path + f'/{phase}'\n",
    "    %mkdir {target_path}\n",
    "    %mkdir {p}\n",
    "    for i, d in tqdm_notebook(enumerate(dl[phase])):\n",
    "        with torch.no_grad():\n",
    "            x = d['bw']\n",
    "            y = d['rgb']\n",
    "            x = x*c_range_scale_factor\n",
    "            fake = models['gen'](x.to(device))\n",
    "            x = x.cpu()*0.5 + 0.5\n",
    "            y = y.cpu()*0.5 + 0.5\n",
    "            fake = fake.cpu()*0.5 + 0.5\n",
    "            # save images\n",
    "            real_images = np.array((y*255.).permute(0, 2, 3, 1)).astype(np.uint8)\n",
    "            fake_images = np.array((fake*255.).permute(0, 2, 3, 1)).astype(np.uint8)\n",
    "            for j in range(len(real_images)):\n",
    "                Image.fromarray(fake_images[j]).save(f'{source_path}/{phase}/{i*bs + j}.jpg')\n",
    "                Image.fromarray(real_images[j]).save(f'{target_path}/{phase}/{i*bs + j}.jpg')\n",
    "            display_result_real_fake(y.cpu(), fake.cpu())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Original model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = get_generator().eval().to('cuda:1')\n",
    "for x in gen.parameters():  \n",
    "    x.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "size = (128, 128)\n",
    "bs = 32\n",
    "ds = Data(fileDict['train'], 'val', size)\n",
    "train = DataLoader(ds, batch_size=bs)\n",
    "ds = Data(fileDict['val'], 'val', size)\n",
    "val = DataLoader(ds, batch_size=bs)\n",
    "ds = Data(fileDict['test'], 'val', size)\n",
    "test = DataLoader(ds, batch_size=bs)\n",
    "dl = {'Train': train, 'Val': val, 'Test': test}\n",
    "mean = torch.tensor([0.485, 0.456, 0.406], device='cuda:1')[None, :, None, None]\n",
    "std = torch.tensor([0.229, 0.224, 0.225], device='cuda:1')[None, :, None, None]\n",
    "c_range_scale_factor = 0.95\n",
    "discarded = (1 - c_range_scale_factor)\n",
    "print(f'Scaling from 0~255 to {round(discarded*255./2, 2)} ~ {round(255. - discarded*255./2, 2)}')\n",
    "for phase in ['Train', 'Val', 'Test']:\n",
    "    print(phase)\n",
    "    source_path = f'./source_images_deoldify/{size[0]}'\n",
    "    p = source_path + f'/{phase}'\n",
    "    %mkdir {source_path}\n",
    "    %mkdir {p}\n",
    "    target_path = f'./target_images_deoldify/{size[0]}'\n",
    "    p = target_path + f'/{phase}'\n",
    "    %mkdir {target_path}\n",
    "    %mkdir {p}\n",
    "    for i, d in tqdm_notebook(enumerate(dl[phase])):\n",
    "        with torch.no_grad():\n",
    "            x = d['bw']\n",
    "            y = d['rgb']\n",
    "            x = (x*c_range_scale_factor + discarded/2.)\n",
    "            fake = gen((x.to('cuda:1') - mean)/std)*std + mean\n",
    "            # save images\n",
    "            real_images = np.array((y*255.).permute(0, 2, 3, 1)).astype(np.uint8)\n",
    "            fake_images = np.array((fake*255.).permute(0, 2, 3, 1)).astype(np.uint8)\n",
    "            for j in range(len(real_images)):\n",
    "                Image.fromarray(fake_images[j]).save(f'{source_path}/{phase}/{i*bs + j}.jpg')\n",
    "                Image.fromarray(real_images[j]).save(f'{target_path}/{phase}/{i*bs + j}.jpg')\n",
    "            display_result_real_fake(y.cpu(), fake.cpu())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics (mse, psnr, ssim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in fileDict.values():\n",
    "    print(len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "from tqdm.notebook import tqdm_notebook\n",
    "from skimage import measure, metrics\n",
    "from collections import defaultdict\n",
    "from skimage.color import rgb2gray\n",
    "from matplotlib import image as mpimg\n",
    "import numpy as np\n",
    "import cv2\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metric_values(source_img_folder, target_img_folder, p=0.7):\n",
    "    result = defaultdict(list)\n",
    "    files_num = len(glob.glob(os.path.join(source_img_folder, '*.jpg')))\n",
    "    for i in tqdm_notebook(range(files_num)):\n",
    "        x_img = mpimg.imread(os.path.join(source_img_folder, f'{i}.jpg'))\n",
    "        y_img = mpimg.imread(os.path.join(target_img_folder, f'{i}.jpg'))\n",
    "        mse = metrics.mean_squared_error(y_img / 255., x_img / 255.)\n",
    "        psnr = metrics.peak_signal_noise_ratio(cv2.cvtColor(y_img, cv2.COLOR_RGB2GRAY), cv2.cvtColor(x_img, cv2.COLOR_RGB2GRAY), data_range=255.)\n",
    "        ssim = metrics.structural_similarity(y_img, x_img, data_range=255., multichannel=True)\n",
    "        result['mse'].append(mse)\n",
    "        result['psnr'].append(psnr)\n",
    "        result['ssim'].append(ssim)\n",
    "    result['mse'] = np.array(result['mse'])\n",
    "    result['psnr'] = np.array(result['psnr'])\n",
    "    result['ssim'] = np.array(result['ssim'])\n",
    "    if p < 1.0:\n",
    "        for i in result.keys():\n",
    "            low = np.percentile(result[i], (1 - p)/2*100)\n",
    "            high = np.percentile(result[i], (1 - (1 - p)/2)*100)\n",
    "            filt = (result[i] >= low) & (result[i] < high)\n",
    "            result[i] = result[i][filt]\n",
    "    print({key: value.mean() for key, value in result.items()})\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phase = {'Train': None, 'Val': None, 'Test': None}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in phase.keys():\n",
    "    print(p)\n",
    "    phase[p] = calculate_metric_values(f'./source_images_coco/128/{p}', f'./target_images_coco/128/{p}', p=1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FID Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fid_score.fid_score import FidScore\n",
    "from IPython.display import clear_output\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "phase = ['Train', 'Val', 'Test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Train': 16.373062245510766, 'Val': 23.958864841511456, 'Test': 16.66251215843073}\n"
     ]
    }
   ],
   "source": [
    "score = {}\n",
    "for p in phase:\n",
    "    print(p)\n",
    "    fid = FidScore([f'./source_images_coco/128/{p}', f'./target_images_coco/128/{p}'], torch.device('cuda:0'))\n",
    "    score[p] = fid.calculate_fid_score()\n",
    "    print('*'*100)\n",
    "    clear_output()\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchinfo import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "14,059,056"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "175,722,165"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Train\n",
    "100%\n",
    "118060/118060 [10:21<00:00, 198.20it/s]\n",
    "{'mse': 0.006903416175397951, 'psnr': 28.658236274071477, 'ssim': 0.8886039778122758}\n",
    "Val\n",
    "100%\n",
    "4990/4990 [00:26<00:00, 190.14it/s]\n",
    "{'mse': 0.006947367393163914, 'psnr': 28.652167830791115, 'ssim': 0.8888692060044308}\n",
    "Test\n",
    "100%\n",
    "40640/40640 [03:29<00:00, 192.37it/s]\n",
    "{'mse': 0.006935229977418596, 'psnr': 28.669569631673916, 'ssim': 0.8887820486019322}\n",
    "{'Train': 16.373062245510766, 'Val': 23.958864841511456, 'Test': 16.66251215843073}"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "pytorch-gpu.1-8.m69",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-8:m69"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
